{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acb4f9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting inflect\n",
      "  Downloading inflect-5.6.0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: inflect\n",
      "Successfully installed inflect-5.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install inflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31628744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\hezza\\anaconda3\\lib\\site-packages (from textblob) (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\hezza\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\hezza\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hezza\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hezza\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hezza\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.4)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "227266da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting elasticsearch==7.0.0Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading elasticsearch-7.0.0-py2.py3-none-any.whl (80 kB)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in c:\\users\\hezza\\anaconda3\\lib\\site-packages (from elasticsearch==7.0.0) (1.26.7)\n",
      "Installing collected packages: elasticsearch\n",
      "Successfully installed elasticsearch-7.0.0\n"
     ]
    }
   ],
   "source": [
    "pip install elasticsearch==7.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eed779c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import inflect\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords # used for preprocessing\n",
    "from nltk.stem import WordNetLemmatizer # used for preprocessing\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from operator import add\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a631a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "# create instance of elasticsearch\n",
    "es = Elasticsearch([{'host':'localhost','port':9200}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a47c6e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c0c2d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'skype'\n",
    "# ------------------- your consummer ------------------- -----\n",
    "consumer = KafkaConsumer(\n",
    "    topic_name,\n",
    "     bootstrap_servers=['localhost:9092'],\n",
    "     auto_offset_reset='latest',\n",
    "     enable_auto_commit=True,\n",
    "     auto_commit_interval_ms =  5000,\n",
    "     fetch_max_bytes = 128,\n",
    "     max_poll_records = 100,\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fde3a641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------Traitement du data -----------------------\n",
    "\n",
    "def remove_urls(text):\n",
    "    new_text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^A-Za-z# \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
    "    return new_text\n",
    "\n",
    "# make all text lowercase\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# make all text Uppercase\n",
    "\n",
    "def text_uppercase(text):\n",
    "    return text.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c8ec8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------delete numbers from text ---------\n",
    "\n",
    "def remove_numbers(text):\n",
    "    result = re.sub(r'\\d+', '', text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3104d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------- delete ponctuation existent on text ----------\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "\n",
    "# ----------------convert list to words ---------------\n",
    "def tokenize(text):\n",
    "    text = word_tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04cdb1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------- delete words non significant ---------\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    text = [i for i in text if not i in stop_words]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbd81647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------get the origins of words--------------\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(text):\n",
    "    text = [lemmatizer.lemmatize(token) for token in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a84359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------remplace all number with word notation-----------\n",
    "def numbers_to_char(text):\n",
    "    wordtoken = nltk.word_tokenize(text)\n",
    "    p = inflect.engine()\n",
    "    new_Text = []\n",
    "    for word in wordtoken:\n",
    "        if word.isdigit():\n",
    "            newword = p.number_to_words(word)\n",
    "            new_Text.append(newword)\n",
    "        else:\n",
    "            new_Text.append(word)\n",
    "    return new_Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eee8d948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------end analyse feedback using textBlob--------------------------------\n",
    "def analyze_sentiment(text):\n",
    "    testimonial = TextBlob(text)\n",
    "    return testimonial.sentiment.polarity\n",
    "def get_sentiment_tuple(sent):\n",
    "    neutral_threshold = 0.05\n",
    "    if sent >= neutral_threshold:       # positive\n",
    "        return (0, 0, 1),\"positive\"\n",
    "    elif sent > -neutral_threshold:     # neutral\n",
    "        return (0, 1, 0),\"neutral\"\n",
    "    else:                               # negative\n",
    "        return (1, 0, 0),\"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa5cbbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "counter=0 \n",
    "def topHashtag(text):\n",
    "    return text.split(\" \")\n",
    "def getNouns(txt):\n",
    "    blob = TextBlob(txt)\n",
    "    print(blob.noun_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d992812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in consumer:\n",
    "        #print(str(1));\n",
    "        time.sleep(2)\n",
    "      \n",
    "        data= text_lowercase(msg.value)\n",
    "        \n",
    "        \n",
    "        #------hashtages----------------------------------------------\n",
    "        dict_data = json.loads(data)\n",
    "        key=\"text\"\n",
    "        \n",
    "        if (key in dict_data.keys()):\n",
    "            print(dict_data[\"text\"])\n",
    "            hashtags = re.findall(\"#[a-zA-Z0-9_]{1,200}\", dict_data[\"text\"])\n",
    "            print(\"hashtags :\",hashtags)\n",
    "       \n",
    "        #---feedback-----------------------------------\n",
    "            tweets=TextBlob(dict_data[\"text\"])\n",
    "            sentiments=analyze_sentiment(dict_data[\"text\"])\n",
    "\n",
    "        #----------best and bad tweets ------------------\n",
    "            frequencetweets= text_lowercase(dict_data[\"text\"])\n",
    "            frequencetweets=remove_urls(frequencetweets)\n",
    "\n",
    "        #----------- frequency of words---------------\n",
    "            filtreData= text_lowercase(dict_data[\"text\"])\n",
    "            filtredData=remove_urls(filtreData)\n",
    "            filtredData=remove_punctuation(filtredData)\n",
    "            filtredData=tokenize(filtredData)\n",
    "            filtredData=remove_stopwords(filtredData)\n",
    "            FrequencesMots=lemmatize(filtredData)\n",
    "  \n",
    "            print(FrequencesMots)\n",
    "            analysentimens,feedback=get_sentiment_tuple(sentiments)\n",
    "            print(\"val :\",analysentimens,\"  sentiments : \",feedback)\n",
    "       \n",
    "        # ------------------- send all result to elastic search --------------------- \n",
    "            \n",
    "            es.index(index=\"tweet_index\" ,\n",
    "                    doc_type=\"test_doc\",\n",
    "                    body={\n",
    "                            \"author\": dict_data[\"user\"][\"screen_name\"],\n",
    "                            \"date\": dict_data[\"created_at\"],\n",
    "                            \"message\": dict_data[\"text\"], \n",
    "                            \"sentimentsPer\":sentiments,\n",
    "                            \"feedback\":feedback,\n",
    "                            \"hashtag\":hashtags,\n",
    "                            \"FrequencMots\":FrequencesMots,\n",
    "                            \"FrequenceTweets\":frequencetweets\n",
    "                    }\n",
    "                )\n",
    "           \n",
    "             \n",
    "        #-------------------------- optional ---------------------\n",
    "        #time.sleep(5)\n",
    "        #print(str(tweets))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c83f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf9263a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
